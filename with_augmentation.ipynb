{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pawel\\anaconda3\\envs\\env_torch\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\Pawel\\anaconda3\\envs\\env_torch\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './knee-osteoarthritis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = f'{DATASET_PATH}/train'\n",
    "VAL_PATH = f'{DATASET_PATH}/val'\n",
    "TEST_PATH = f'{DATASET_PATH}/test'\n",
    "AUTO_TEST_PATH = f'{DATASET_PATH}/auto_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pawel\\anaconda3\\envs\\env_torch\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.dataset.augmented_dataset import get_KneeOsteoarthritis_Edges\n",
    "\n",
    "train_dataset = get_KneeOsteoarthritis_Edges(TRAIN_PATH)\n",
    "val_dataset = get_KneeOsteoarthritis_Edges(VAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256]) torch.Size([1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "row = train_dataset[1]\n",
    "normal_ex = row[0]\n",
    "augmented_ex = row[1]\n",
    "print(normal_ex.shape, augmented_ex.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomModel(nn.Module):\n",
    "#     def __init__(self, num_classes: int = 5, dropout: float = 0.5) -> None:\n",
    "#         super().__init__()\n",
    "        \n",
    "#         BASE_SIZE = 32\n",
    "        \n",
    "#         # Images\n",
    "#         self.images_features = nn.Sequential(\n",
    "#             nn.Conv2d(3, BASE_SIZE, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(BASE_SIZE, BASE_SIZE*2, kernel_size=5, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(BASE_SIZE*2, BASE_SIZE*4, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(BASE_SIZE*4, BASE_SIZE*3, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(BASE_SIZE*3, BASE_SIZE*2, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#         )\n",
    "#         self.images_avgpool = nn.Sequential(\n",
    "#           # nn.AdaptiveAvgPool2d((6, 6)),\n",
    "#           nn.Flatten()\n",
    "#         )\n",
    "#         self.images_classifier = nn.Sequential(\n",
    "#             nn.Dropout(p=dropout),\n",
    "#             nn.Linear(BASE_SIZE*2 * 7 * 7, BASE_SIZE*3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=dropout),\n",
    "#             nn.Linear(BASE_SIZE*3, BASE_SIZE),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(BASE_SIZE, num_classes),\n",
    "#         )\n",
    "        \n",
    "#         EDGES_SIZE = 32\n",
    "#         # Edges\n",
    "#         self.edges_features = nn.Sequential(\n",
    "#             nn.Conv2d(3, EDGES_SIZE, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(EDGES_SIZE, EDGES_SIZE*2, kernel_size=5, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(EDGES_SIZE*2, EDGES_SIZE*4, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(EDGES_SIZE*4, EDGES_SIZE*3, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(EDGES_SIZE*3, EDGES_SIZE*2, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#         )\n",
    "#         self.edges_avgpool = (\n",
    "#           # nn.AdaptiveAvgPool2d((6, 6)),\n",
    "#           nn.Flatten()\n",
    "#         )\n",
    "#         self.edges_classifier = nn.Sequential(\n",
    "#             nn.Dropout(p=dropout),\n",
    "#             nn.Linear(EDGES_SIZE*2 * 7 * 7, EDGES_SIZE*3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=dropout),\n",
    "#             nn.Linear(EDGES_SIZE*3, EDGES_SIZE),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(EDGES_SIZE, num_classes),\n",
    "#         )\n",
    "        \n",
    "#         self.outputCombiner = nn.Sequential(\n",
    "#             nn.Linear(2 * num_classes, num_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, image: torch.Tensor, edges: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "#         images = self.images_features(image)\n",
    "#         # print(images.shape)\n",
    "#         images = self.images_avgpool(images)\n",
    "#         # print(images.shape)\n",
    "#         images = self.images_classifier(images)\n",
    "        \n",
    "#         # Edges\n",
    "#         edges = self.edges_features(image)\n",
    "#         # print(edges.shape)\n",
    "#         edges = self.edges_avgpool(edges)\n",
    "#         # print(edges.shape)\n",
    "#         edges = self.edges_classifier(edges)\n",
    "        \n",
    "#         # Combining outputs\n",
    "#         concated = torch.cat((images, edges), 1)\n",
    "#         res = self.outputCombiner(concated)\n",
    "        \n",
    "#         return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.edgesClassifier = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.Dropout(p=dropout*0.4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Dropout(p=dropout*0.6),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout*0.8),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(32 * 6 * 6, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, image: torch.Tensor, edges: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Edges\n",
    "        edges = self.edgesClassifier(edges)\n",
    "        \n",
    "        return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.models.custom import CustomModel\n",
    "\n",
    "model = CustomModel(3, 0.5)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202723\n",
      "202723\n"
     ]
    }
   ],
   "source": [
    "# print(sum(p.numel() for p in net.classifier.parameters()) ,sum(p.numel() for p in net.edgesClassifier.parameters()) )\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print(sum(p.numel() for p in trainable_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from src.other import getClassesFrequency\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "class_weights = getClassesFrequency(train_dataset)\n",
    "weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "  for param_group in optimizer.param_groups:\n",
    "    return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"images_downscaled_alexnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logger = SummaryWriter(log_dir=f\"logs/experiments/{EXP_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochCounter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.validation import validate\n",
    "\n",
    "def train_many(model, epochs_nr, regularization_type = \"L2\", lambda_reg=0.01):\n",
    "    global epochCounter\n",
    "    \n",
    "    for epoch in range(0, epochs_nr):  # loop over the dataset multiple times\n",
    "        epoch_correct = 0\n",
    "        epoch_samples = 0\n",
    "        epoch_batches = 0\n",
    "        running_loss = 0.0\n",
    "    \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, edges, labels = data\n",
    "            images = images.to(device)\n",
    "            edges = edges.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(images, edges)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Apply L1 regularization\n",
    "            if regularization_type == 'L1':\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss += lambda_reg * l1_norm\n",
    "                \n",
    "            # Apply L2 regularization\n",
    "            elif regularization_type == 'L2':\n",
    "                l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "                loss += lambda_reg * l2_norm\n",
    "                \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Changing outputs (logits) to labels\n",
    "            outputs_clear = outputs.max(1).indices\n",
    "            \n",
    "            epoch_correct += (outputs_clear == labels).float().sum()\n",
    "            epoch_samples += len(outputs)\n",
    "            epoch_batches +=1\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        tAccuracy = epoch_correct / epoch_samples * 100\n",
    "        tLoss = running_loss / epoch_batches\n",
    "        \n",
    "        # Validation\n",
    "        vAccuracy, vLoss = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        logger.add_text(\"REGULARIZATION_TYPE\", regularization_type, global_step=epochCounter)\n",
    "        logger.add_scalar(\"REGULARIZATION_LAMBDA\", lambda_reg, global_step=epochCounter)\n",
    "        logger.add_scalar(\"learning_rate\", get_lr(optimizer), global_step=epochCounter)\n",
    "        \n",
    "        logger.add_scalar(\"Accuracy/train\", tAccuracy, global_step=epochCounter)\n",
    "        logger.add_scalar(\"Loss/train\", tLoss, global_step=epochCounter)\n",
    "        logger.add_scalar(\"Accuracy/validation\", vAccuracy, global_step=epochCounter)\n",
    "        logger.add_scalar(\"Loss/validation\", vLoss, global_step=epochCounter)\n",
    "        \n",
    "        print(f'Epoch {epochCounter}: Training: accuracy: {tAccuracy:.3f}%, loss: {tLoss:.3f}; Validation: accuracy: {vAccuracy:.3f}%, loss: {vLoss:.3f}')\n",
    "        \n",
    "        epochCounter += 1\n",
    "        \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[69], line 37\u001b[0m, in \u001b[0;36mtrain_many\u001b[1;34m(model, epochs_nr, regularization_type, lambda_reg)\u001b[0m\n\u001b[0;32m     34\u001b[0m     l2_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     35\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lambda_reg \u001b[38;5;241m*\u001b[39m l2_norm\n\u001b[1;32m---> 37\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Changing outputs (logits) to labels\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pawel\\anaconda3\\envs\\env_torch\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pawel\\anaconda3\\envs\\env_torch\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pawel\\anaconda3\\envs\\env_torch\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_many(model, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
